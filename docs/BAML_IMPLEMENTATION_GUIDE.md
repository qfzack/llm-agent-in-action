# BAML é›†æˆå®æ–½æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šå®‰è£… BAML

```bash
# 1. å®‰è£… BAML Python åŒ…
pip install baml-py

# 2. éªŒè¯å®‰è£…
baml-cli --version
```

### ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–é¡¹ç›®ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æ˜¯å…¨æ–°é¡¹ç›®ï¼š
```bash
baml init
```

å¦‚æœå·²æœ‰é¡¹ç›®ï¼ˆæˆ‘ä»¬çš„æƒ…å†µï¼‰ï¼Œå·²ç»å‡†å¤‡å¥½äº† `baml_src/` ç›®å½•ï¼Œè·³è¿‡æ­¤æ­¥ã€‚

### ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Python å®¢æˆ·ç«¯

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
baml-cli generate

# è¿™ä¼šç”Ÿæˆ baml_client/ ç›®å½•ï¼ŒåŒ…å«ç±»å‹å®‰å…¨çš„ Python ä»£ç 
```

### ç¬¬å››æ­¥ï¼šé…ç½®ç¯å¢ƒå˜é‡

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰ï¼š

```env
# OpenAI (å¿…éœ€)
OPENAI_API_KEY=your_openai_key
OPENAI_API_BASE=https://api.openai.com/v1

# Anthropic Claude (å¯é€‰)
ANTHROPIC_API_KEY=your_anthropic_key

# Google Gemini (å¯é€‰)
GEMINI_API_KEY=your_gemini_key
```

### ç¬¬äº”æ­¥ï¼šæµ‹è¯• BAML

```bash
# è¿è¡Œ BAML æµ‹è¯•ï¼ˆå¦‚æœæœ‰ï¼‰
baml test

# æˆ–è€…è¿è¡Œç¤ºä¾‹ä»£ç 
python -m src.core.agent_baml
```

---

## è¯¦ç»†é›†æˆæ­¥éª¤

### 1. é¡¹ç›®ç»“æ„

å½“å‰å·²åˆ›å»ºçš„ BAML æ–‡ä»¶ï¼š

```
baml_src/
â”œâ”€â”€ clients.baml          # âœ… LLM å®¢æˆ·ç«¯é…ç½®
â”œâ”€â”€ types.baml            # âœ… æ•°æ®ç±»å‹å®šä¹‰
â””â”€â”€ functions/
    â”œâ”€â”€ rag.baml          # âœ… RAG é—®ç­”å‡½æ•°
    â”œâ”€â”€ analysis.baml     # âœ… æ–‡æ¡£åˆ†æå‡½æ•°
    â””â”€â”€ reasoning.baml    # âœ… æ¨ç†å‡½æ•°
```

ç”Ÿæˆåä¼šæœ‰ï¼š
```
baml_client/             # è‡ªåŠ¨ç”Ÿæˆçš„ Python å®¢æˆ·ç«¯
â”œâ”€â”€ __init__.py
â”œâ”€â”€ types.py            # ç±»å‹å®šä¹‰
â””â”€â”€ ...
```

### 2. ä½¿ç”¨ BAML Agent

#### æ–¹å¼ 1ï¼šç›´æ¥ä½¿ç”¨ï¼ˆæ¨èç”¨äºæ–°åŠŸèƒ½ï¼‰

```python
from baml_client import b
from baml_client.types import ChatResponse
import asyncio

async def main():
    # è°ƒç”¨ BAML å‡½æ•°
    response: ChatResponse = await b.RAGChat(
        query="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        context="æœºå™¨å­¦ä¹ æ˜¯...",
        has_context=True
    )
    
    # ç±»å‹å®‰å…¨çš„è®¿é—®
    print(response.answer)
    print(response.confidence)
    print(response.category)
    for source in response.sources:
        print(source)

asyncio.run(main())
```

#### æ–¹å¼ 2ï¼šé›†æˆåˆ°ç°æœ‰ Agent

å·²åˆ›å»º `src/core/agent_baml.py`ï¼Œä½¿ç”¨æ–¹å¼ï¼š

```python
from src.core.agent_baml import BAMLAgent
from src.services.vector_store import VectorStore
from src.core.config import settings
import asyncio

async def main():
    vector_store = VectorStore(settings.vector_db_path)
    agent = BAMLAgent(vector_store)
    
    response = await agent.chat("ä½ çš„é—®é¢˜")
    print(response)

asyncio.run(main())
```

### 3. åˆ›å»º CLI å·¥å…·ï¼ˆBAML ç‰ˆæœ¬ï¼‰

åˆ›å»º `src/cli/cli_baml.py`ï¼š

```python
"""ä½¿ç”¨ BAML çš„å‘½ä»¤è¡Œå·¥å…·"""
import asyncio
from ..core.agent_baml import BAMLAgent
from ..services.vector_store import VectorStore
from ..core.config import settings

async def interactive_query():
    """äº¤äº’å¼é—®ç­”ï¼ˆBAML ç‰ˆæœ¬ï¼‰"""
    print("=== BAML Agent äº¤äº’å¼é—®ç­” ===")
    print("è¾“å…¥ 'quit' é€€å‡º\n")
    
    vector_store = VectorStore(settings.vector_db_path)
    agent = BAMLAgent(vector_store)
    
    conversation_history = []
    
    while True:
        query = input("æ‚¨: ").strip()
        if query.lower() in ['quit', 'exit']:
            break
        
        # ä½¿ç”¨ BAML Agent
        result = await agent.chat(query, conversation_history)
        
        print(f"\nAI: {result['answer']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
        print(f"åˆ†ç±»: {result['category']}")
        
        if result.get('sources'):
            print("\næ¥æº:")
            for source in result['sources']:
                print(f"  - {source}")
        
        # æ›´æ–°å†å²
        conversation_history.append({
            "role": "user",
            "content": query
        })
        conversation_history.append({
            "role": "assistant", 
            "content": result['answer']
        })
        
        print()

if __name__ == "__main__":
    asyncio.run(interactive_query())
```

è¿è¡Œï¼š
```bash
python -m src.cli.cli_baml
```

### 4. åˆ›å»º API ç«¯ç‚¹ï¼ˆBAML ç‰ˆæœ¬ï¼‰

åˆ›å»º `src/api/main_baml.py`ï¼š

```python
"""ä½¿ç”¨ BAML çš„ FastAPI æœåŠ¡"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict
import asyncio

from ..core.agent_baml import BAMLAgent
from ..services.vector_store import VectorStore
from ..core.config import settings

app = FastAPI(title="BAML Agent API")

# åˆå§‹åŒ–
vector_store = VectorStore(settings.vector_db_path)
agent = BAMLAgent(vector_store)

class QueryRequest(BaseModel):
    query: str
    conversation_history: Optional[List[Dict[str, str]]] = None

class QueryResponse(BaseModel):
    answer: str
    confidence: float
    has_context: bool
    sources: List[str]
    category: str

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """é—®ç­”æ¥å£ï¼ˆä½¿ç”¨ BAMLï¼‰"""
    try:
        result = await agent.chat(
            request.query,
            request.conversation_history
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze")
async def analyze_document(text: str):
    """æ–‡æ¡£åˆ†ææ¥å£"""
    try:
        result = await agent.analyze_document(text)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health_check():
    return {"status": "healthy", "agent": "BAML"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

è¿è¡Œï¼š
```bash
# ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
python -m src.api.main_baml
# è®¿é—® http://localhost:8001/docs
```

---

## æµ‹è¯• BAML

### åˆ›å»ºæµ‹è¯•æ–‡ä»¶

åˆ›å»º `baml_src/tests/rag_test.baml`ï¼š

```baml
test "Basic RAG with context" {
  functions [RAGChat]
  args {
    query "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    context "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
    has_context true
  }
  assert {
    output.answer is string
    output.confidence >= 0.5
    output.has_context == true
  }
}

test "RAG without context" {
  functions [RAGChat]
  args {
    query "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    context ""
    has_context false
  }
  assert {
    output.has_context == false
  }
}

test "Document analysis" {
  functions [AnalyzeDocument]
  args {
    text "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚"
  }
  assert {
    output.summary is string
    output.key_points.length >= 1
    output.complexity in [Complexity.Easy, Complexity.Medium, Complexity.Hard]
  }
}
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
baml test
```

---

## åˆ‡æ¢ LLM æ¨¡å‹

### æ–¹æ³• 1ï¼šä¿®æ”¹å‡½æ•°å®šä¹‰

åœ¨ `baml_src/functions/rag.baml` ä¸­ï¼š

```baml
function RAGChat(...) -> ChatResponse {
  client GPT4  // æ”¹æˆ GPT35Turboã€Claude æˆ– Gemini
  prompt #"..."#
}
```

### æ–¹æ³• 2ï¼šè¿è¡Œæ—¶é€‰æ‹©

```python
from baml_client import b

# ä½¿ç”¨ä¸åŒçš„å®¢æˆ·ç«¯
response = await b.RAGChat.with_client("Claude")(
    query=query,
    context=context,
    has_context=True
)
```

---

## å¯¹æ¯”ï¼šæ—§ vs æ–°

### æ—§æ–¹å¼ï¼ˆåŸå§‹ä»£ç ï¼‰

```python
# ç±»å‹ä¸å®‰å…¨ï¼Œå®¹æ˜“å‡ºé”™
result = agent.chat(query)
answer = result["answer"]  # å¯èƒ½ KeyError
confidence = result.get("confidence", 0)  # æ²¡æœ‰ç±»å‹æ£€æŸ¥

# Prompt åœ¨ä»£ç é‡Œ
def generate_prompt(query, context):
    return f"""ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹...
    {context}
    {query}
    """

# åˆ‡æ¢æ¨¡å‹éœ€è¦æ”¹ä»£ç 
self.llm_adapter = LLMFactory.create_adapter(
    provider="openai",  # è¦æ”¹è¿™é‡Œ
    ...
)
```

### æ–°æ–¹å¼ï¼ˆBAMLï¼‰

```python
# ç±»å‹å®‰å…¨
response: ChatResponse = await b.RAGChat(
    query=query,
    context=context,
    has_context=True
)
print(response.answer)  # IDE è‡ªåŠ¨å®Œæˆ
print(response.confidence)  # ç±»å‹æ£€æŸ¥

# Prompt åœ¨ .baml æ–‡ä»¶ä¸­ç®¡ç†
# åˆ‡æ¢æ¨¡å‹åªéœ€æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ client åç§°
```

---

## æ€§èƒ½ä¼˜åŒ–

### å¹¶è¡Œè°ƒç”¨

```python
import asyncio

# åŒæ—¶è°ƒç”¨å¤šä¸ª BAML å‡½æ•°
results = await asyncio.gather(
    b.RAGChat(query1, context1, True),
    b.RAGChat(query2, context2, True),
    b.AnalyzeDocument(text)
)
```

### æµå¼è¾“å‡º

```python
# BAML æ”¯æŒæµå¼å“åº”
async for chunk in b.RAGChat.stream(query, context, True):
    print(chunk, end="", flush=True)
```

---

## ç›‘æ§å’Œè°ƒè¯•

### BAML Studio

```bash
# å¯åŠ¨ BAML Studioï¼ˆå¯è§†åŒ–ç•Œé¢ï¼‰
baml studio
```

åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ï¼š
- æ‰€æœ‰ LLM è°ƒç”¨è®°å½•
- Prompt å’Œå“åº”
- Token ä½¿ç”¨é‡
- å“åº”æ—¶é—´
- æˆæœ¬ç»Ÿè®¡

### æ—¥å¿—

```python
import logging
from baml_client import b

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

response = await b.RAGChat(query, context, True)
# ä¼šè‡ªåŠ¨è®°å½•è°ƒç”¨è¯¦æƒ…
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ä¿æŒå‘åå…¼å®¹ï¼Ÿ

**A**: åŒæ—¶ä¿ç•™ä¸¤ä¸ª Agentï¼š

```python
# æ—§ Agentï¼ˆä¿æŒä¸å˜ï¼‰
from src.core.agent import AIAgent

# æ–° BAML Agent
from src.core.agent_baml import BAMLAgent

# åœ¨ API ä¸­æä¾›ä¸¤ä¸ªç«¯ç‚¹
@app.post("/query")  # æ—§æ¥å£
async def query_old():
    agent = AIAgent(vector_store)
    return agent.chat(query)

@app.post("/query-baml")  # æ–°æ¥å£
async def query_new():
    agent = BAMLAgent(vector_store)
    return await agent.chat(query)
```

### Q2: BAML æ”¯æŒå“ªäº› LLMï¼Ÿ

**A**: æ”¯æŒä¸»æµ LLMï¼š
- OpenAI (GPT-3.5, GPT-4, GPT-4o)
- Anthropic (Claude)
- Google (Gemini)
- Ollama (æœ¬åœ°æ¨¡å‹)
- Azure OpenAI
- è‡ªå®šä¹‰ OpenAI å…¼å®¹ API

### Q3: BAML ä¼šå¢åŠ å»¶è¿Ÿå—ï¼Ÿ

**A**: å‡ ä¹æ²¡æœ‰ã€‚BAML åªæ˜¯åœ¨è°ƒç”¨å‰ååšç±»å‹è½¬æ¢å’ŒéªŒè¯ï¼Œå¼€é”€æå°ï¼ˆ< 1msï¼‰ã€‚

### Q4: å¦‚ä½•è°ƒè¯• Promptï¼Ÿ

**A**: 
1. ä½¿ç”¨ `baml studio` æŸ¥çœ‹å®é™…å‘é€çš„ Prompt
2. åœ¨ `.baml` æ–‡ä»¶ä¸­è¿­ä»£ä¿®æ”¹
3. ä½¿ç”¨ `baml test` éªŒè¯æ•ˆæœ

### Q5: ç”Ÿæˆçš„ä»£ç éœ€è¦æäº¤å—ï¼Ÿ

**A**: 
```bash
# å»ºè®®æ·»åŠ åˆ° .gitignore
echo "baml_client/" >> .gitignore

# åœ¨ CI/CD ä¸­è‡ªåŠ¨ç”Ÿæˆ
baml-cli generate
```

---

## æ›´æ–° requirements.txt

```bash
# æ·»åŠ  BAML ä¾èµ–
echo "baml-py>=0.40.0" >> requirements.txt

# é‡æ–°å®‰è£…
pip install -r requirements.txt
```

---

## ä¸‹ä¸€æ­¥

### ç«‹å³å°è¯•

1. **å®‰è£… BAML**
   ```bash
   pip install baml-py
   ```

2. **ç”Ÿæˆå®¢æˆ·ç«¯**
   ```bash
   baml-cli generate
   ```

3. **è¿è¡Œç¤ºä¾‹**
   ```bash
   python -m src.core.agent_baml
   ```

### æ¸è¿›è¿ç§»

1. **Week 1**: ä¸ºæ–°åŠŸèƒ½ä½¿ç”¨ BAML
2. **Week 2**: è¿ç§»æ ¸å¿ƒé—®ç­”åŠŸèƒ½
3. **Week 3**: è¿ç§»æ–‡æ¡£åˆ†æåŠŸèƒ½
4. **Week 4**: å®Œå…¨è¿ç§»åˆ° BAML

### å­¦ä¹ èµ„æº

- ğŸ“š [BAML å®˜æ–¹æ–‡æ¡£](https://docs.boundaryml.com)
- ğŸ® [äº¤äº’å¼æ•™ç¨‹](https://playground.boundaryml.com)
- ğŸ’¬ [Discord ç¤¾åŒº](https://discord.gg/boundaryml)
- ğŸ“¹ [è§†é¢‘æ•™ç¨‹](https://youtube.com/@boundaryml)

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹ä½¿ç”¨ BAML è®©ä½ çš„ LLM Agent æ›´å¼ºå¤§ï¼** ğŸš€
